---
title: "Workflow"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(citr)
library(viridis)
library(kableExtra)
library(readr)
library(plotly)
library(here)
```

Work as part of the [*Collaborative Research Centre 990: Ecological and Socioeconomic Functions of Tropical Lowland Rainforest Transformation Systems (Sumatra, Indonesia)*](https://www.uni-goettingen.de/en/310995.html)

*Project Group B - Biota and ecosystem services*

*B05 - Land use patterns in Jambi - quantification of structure, heterogeneity and changes of vegetation and land use as a basis for the explanation of ecological and socioeconomic functions*

-----------------

# Introduction


## Previous studies

Many studies attempted to produce land-use/land-cover (LuLc) maps for Jambi province. Nevertheless, these maps do not meet project demands for different reasons:

- do not cover entire province 
- classes are delineated visually with coarse resolution
- are outdated

e.g. [@Ekadinata2011; @Melati2017; @Nurwanda2016; @SooChinLiew2003]

-----------------

## Objectives of this study

In order to serve as reference for further research questions in the project a number of demands have been defined, the produced map should:

- cover the entire province
- be up-to-date (ideally 2018/19)
- have similar classes to other classifications to make it comparable
- have a focus on oil palm plantations
- have a reasonable accuracy

Ideally a similar map with same classes can be produced for past years to:

- enable change analysis
- detect hotspots of forest conversion
- make predictions for future development


-----------------

# Methods

The classification process is done with Google Earth Engine (GEE) and R. While the reference data was delineated in GEE, images from Bing Maps and Google Earth Pro were also used for visual interpretation of lulc-classes. As model tuning is not possible in GEE the reference data was exported to R in order to find the best model parameters. The classification itself was then done in GEE, using model parameters defined in R. 

Reference Data | Model Tuning | Classification
--- | --- | ---


```{r, echo=FALSE, out.width = "75px"}
knitr::include_graphics("https://earthengine.google.com/static/images/earth-engine-logo.png")
```
`r icon::fa("plus")`
```{r, echo=FALSE, out.width = "50px"}
knitr::include_graphics("https://www.macupdate.com/images/icons512/55148.png")
```
`r icon::fa("plus")`
```{r, echo=FALSE, out.width = "50px"}
knitr::include_graphics("https://secure.gravatar.com/avatar/d466c4c764058f633599bd5b53792d48?s=512&r=g&d=retro")
```
&emsp;&emsp; `r icon::fa("angle-double-right", size = 2)` &emsp;
```{r, echo=FALSE, out.width = "75px"}
knitr::include_graphics("https://www.zwodnik.com/media/cache/e9/84/e9846013e4388ce0708810344fef7b4c.png")
```
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; `r icon::fa("angle-double-right", size = 2)` &emsp;
```{r, echo=FALSE, out.width = "150px"}
knitr::include_graphics("https://static1.squarespace.com/static/56df0fad746fb9352d1fc4de/t/5ab02ac76d2a73ff3a9380e4/1521494491356/?format=1500w")
```


-----------------

## Reference Data

### LuLc-Classes in previous studies

Previous studies on land-cover and land-use in Jambi used different classification keys. Some classes were classified as sub-categories in other studies, while other classes were merged or were not assessed.  
@Melati2017 used different classification schemes for different purposes (areas) by merging the 22 classes differentiated by the Ministry of Forestry. @Stolle2003 also used many classes (14), while @Sambodo2018, @Nurwanda2016, and @Ekadinata2011 merged some of the classes into broarder categories. 

```{r read_classes, message=FALSE, warning=FALSE}
classes <- read_csv(here("raw_data/classification-classes.csv"), na = "NA")
dt_classes <- classes 

names(dt_classes)[1] <- paste0(names(dt_classes)[1], 
                                footnote_marker_symbol(1))
dt_classes %>%
  kable(escape = F) %>%
  kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), font_size = 12, fixed_thead = T) %>%
  column_spec(1:9, width_min = "15em", italic = TRUE) %>%
  collapse_rows(columns = 1:9, valign = "top") %>%
  scroll_box(width = "100%") %>%
  footnote(symbol =  "as used in Melati (2017)" )

```

-----------------

### LuLc-Classes in this study {.tabset .tabset-pills}

As @Melati2017 suggests oil palm plantations were differentiated in mature and immature plantations.
Class Rubber was not classified seperately since other studies had no success trying this (e.g. @Melati2017) and because rubber trees could not be differentiated visually in high resolution imagery for reference data creation.

#### water {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_water.jpg?raw=true")
```
**Definition:**  
Area covered permanently by water (short variations in dry season might occour).
e.g. river, ocean, lake, pond

#### primary forest {-} 
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_primary-forest.jpg?raw=true")
```
**Definition:**  
Area covered by dense forest without detectable disturbances. Often with heterogenious stand structure and mainly further away from villages.

#### secondary forest {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_secondary-forest.jpg?raw=true")
```
**Definition:**  
Area covered by forest with multiple tree species, which is used by the people. Skidding trails might be visible, sometimes intercropped with other tree species (e.g. jungle rubber). Forests close to cities have a high chance to fall in this category.

#### oilpalm mature {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_oilpalm-mature.jpg?raw=true")
```
**Definition:**  
Productive oil palm plantations, where crown cover is more than 50% due to larger plants and closing canopy. Usually these plantations are older than ? years.

#### oilpalm immature {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_oilpalm-immature.jpg?raw=true")
```
**Definition:**  
Oil palm plantations in the early stage after planting. Plants are still small and crowns cover less than 50% of underlying soil. 

#### bush shrub {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_bush-shrub.jpg?raw=true")
```
**Definition:**  
Area covered by low vegetation (<5m), single trees might occour but no forest condition is present. Mainly covered by grass and small shrubs. This is often the case after areas were burned down.

#### plantation forest {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_plantation-forest.jpg?raw=true")
```
**Definition:**  
Area covered by trees of mainly one species (monoculture), which is planted as a timber production forest. 
e.g. Acacia plantation, monoculture rubber plantation

#### burned cleared {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_burned-cleared.jpg?raw=true")
```
**Definition:**  
Area which was burned down and where no or only very sparse vegetation has recovered in the meantime.

#### urban buildings {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_urban-buildings.jpg?raw=true")
```
**Definition:**  
Areas covered by houses, streets, industry and other sealed surfaces.

#### coconut plantation {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_coconut-plantation.jpg?raw=true")
```
**Definition:**  
Area covoered by coconut monoculture plantations. These plantations look similar to oil palm plantations from aerial images but are planted in smaller stripes in coastal areas.

#### rice {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_rice.jpg?raw=true")
```
Definition: Agricultural areas for rice production. These areas are covered seasonal with water. 

#### tea plantation {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/examples/ge_tea-plantation.jpg?raw=true")
```
**Definition:**  
Agricultural areas used for tea production. Occours only in the upland areas of Jambi.  



### Data collection

Reference data is delineated in GEE with support of high resolution imagery from GE Pro and Bing Maps. The following conditions were met for data collection:

- medium to large sized polygons   
*(more efficient but also less distributed with higher intracorrelation)*  
- cover only one LuLc-class  
*(avoid mixed pixels and false references)*
- based on high quality images from 2018/19  
*(up-to-date mapping)*
- have a low chance of further LuLc-changes recently  
*(avoid false references from changed classes)*
- distributed in the entire region  
*(cover regional variability)*


```{r figs, echo=FALSE, out.width = "100%", fig.cap = "sample of reference data collected in GEE"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/presentation/reference_banner.jpg?raw=true")
```
[see in GEE](https://code.earthengine.google.com/5a3241a7c5d90f1670bf1b3c910ca701)


To generate the actual reference data a stratified sample of 1500 randomly distributed reference pixels per class was extracted from these polygons. In total 12000 reference samples are generated, which are then devided in training and validation data. 

[`r icon::fa("code")` Code](https://wiesehahn.github.io/crc990_b05/ee_scripts.html#11_create_reference_data)

-----------------

## Classification

A supervised classification algorithm with good performance should be used in combination with informative predictor variables.

**input variables**

A combination of Sentinel-1 (radar) and Sentinel-2 (optical) imagery is used, from which a reduced number of bands and indices is needed as predictor variables. Since GEE does not provide the ability of model tuning these steps are done in R.

**classification model**

Decision tree models are used widely for classification problems, preliminary tests indicated that Random Forest (RF) algorithm performed better than Classification And Regression Trees (CART). Hence, a RF model was trained to classify Land-use and Land-cover. 

**classification key**

see previous section on [LuLc-Classes in this study]

-----------------

### Model Tuning

A number of model parameters can be changed to achieve different results. Model tuning changes these parameters to get optimal results. Overfitting might be a problem. Parameters to be changed in all classifiers are input variables (bands and indices in our case) and training data. 
The random forest classifier implemented in GEE uses following parameters which can be tuned:  

- numberOfTrees (Integer, default: 1):
The number of Rifle decision trees to create per class.

- variablesPerSplit (Integer, default: 0):
The number of variables per split. If set to 0 (default), defaults to the square root of the number of variables.

- minLeafPopulation (Integer, default: 1):
The minimum size of a terminal node.

- bagFraction (Float, default: 0.5):
The fraction of input to bag per tree.


#### Gridsearch Results

Since model tuning is not implemented in GEE the reference data was imported in R and a gridsearch approach was applied to calculate performance metrics for different combinations of input parameters.  
The *number of trees* was held constant at a value of 500 because generally it is assumed that more trees give better performance. On the other side larger numbers also mean more processing time and chances to produce time-outs on the GEE server increase.  
Parameters for which a gridsearch was done are the *number of variables per split* (called mtry in R) and the *minimum terminal node size*. 

```{r read, message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(caret)
library(kableExtra)

# read reference data exported by google earth engine
ref <- read_csv(here("raw_data/reference/stratified-reference_20190521.csv"),
                col_types = cols(.geo = col_skip(), 
                                 class = col_factor(levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "10", "11", "12")), 
                                 latitude_209564535 = col_skip(), 
                                 longitude_209564535 = col_skip(), 
                                 `system:index` = col_skip()))

# split in train and test data
index <- createDataPartition(y = ref$class, p = .7, list = FALSE)
training <- ref[index, ]
testing <- ref[-index, ]
```


```{r gridsearch}
# create random forest model
filename = here("output_data/model/rf_fit.rds")
if (file.exists(filename)){
  
  rf_fit <- readRDS(filename)
  
} else{
  
  # specify that the resampling method is 
  fit_control <- trainControl(## 10-fold CV
    method = "cv",
    number = 10)
  
  # define a grid of parameter options to try
  rf_grid <- expand.grid(mtry = c(2, 3, 4, 6, 8),
                         splitrule = c("gini"),
                         min.node.size = c(1, 5, 10))
  
  # run a random forest model
  set.seed(825)
  library(doParallel)
  cl <- makePSOCKcluster(4)
  registerDoParallel(cl)
  
  # fit the model with the parameter grid
  rf_fit <- train(class ~ ., 
                  data = training, 
                  method = "ranger",
                  importance = "impurity" ,
                  trControl = fit_control,
                  # provide a grid of parameters
                  tuneGrid = rf_grid)
  
  stopCluster(cl)
  
  # save model
  saveRDS(rf_fit, filename)
  
}
```


```{r gridsearch_result}
print(rf_fit)
```


```{r gridsearch_result2, fig.cap= "Gridsearch result for number of predictor variables and minimal node size"}
# ggplot(data= rf_fit$results) + 
#   geom_line(aes(x=mtry, y=Accuracy, color= as.factor(min.node.size))) + 
#   geom_point(aes(x=mtry, y=Accuracy, color= as.factor(min.node.size)), size = 3) +
#   scale_color_viridis(discrete = TRUE, option = "D", name="Minimal Node Size") +
#   labs(x = "Randomly Selected Predictors",
#        y = "Accuracy (Cross-Validation)") +
#     theme_classic() +
#   theme(legend.position="bottom") 

rf_fit$results %>%
  plot_ly(x= ~mtry) %>%
  add_trace(y = ~Accuracy, 
            type = "scatter",
            mode = 'lines+markers', 
            color= ~ as.factor(min.node.size), colors = viridis_pal(option = "D")(3),
            error_y = ~list(array = AccuracySD),
            text = ~paste("Accuracy: ", Accuracy, "<br>AccuracySD:", AccuracySD) 
            ) %>%
  layout(xaxis = list(title="Randomly Selected Predictors"),
         yaxis = list(title="Accuracy (Cross-Validation)")) 

```

As we can see the differences are not very pronounced. Hence, a simpler model results in similar performance. Based on these results we can simplify our model in terms of minimal node size and variables per split without deteriorating the results.

-----------------

#### Model Simplification

Here we search for the simplest model without deteriorating the performance (max 2% difference to best model).

```{r gridsearch_result3}
# get simplest model with similar accuracy
whichTwoPct <- tolerance(rf_fit$results, metric = "Accuracy", 
                         tol = 2, maximize = TRUE)  

rf_fit$results[whichTwoPct,]
```

-----------------


#### Model Validation

To have a closer look at the model performance the validation data is classified with the best model obtained by gridsearch. Its confusion matrix is calculated as a value of prediction accuracy.

<br>

**Error Matrix**
```{r gridsearch_validation}
# validation
testing.pred <- predict(rf_fit, newdata = testing[1:19], probability= F)
cm<- confusionMatrix(data = testing.pred, reference = testing$class)

cm.df <- as.data.frame.matrix(cm$table)

cm.df %>%
  
  mutate_all(~ifelse(. > max(cm.df[1:12])*0.75,
                  cell_spec(., "html", color = "white", bold = T, background = "green"),
                  ifelse(. > 0,
                         cell_spec(., color = "white", bold = T, background = spec_color(., option = "A", direction= -1, begin = 0.3, scale_from = c(0,53))),
                         cell_spec(., "html", color = "grey")
                         )
                  )
             ) %>%
  
  mutate(class = cell_spec(c("water - 0","primary forest - 1","secondary forest - 2","mature oilpalm - 3", "immature oilpalm - 4","shrubland - 5","plantation forest - 6","burned / cleared - 7","urban buildings - 8","coconut plantation - 10","rice - 11","tea plantation - 12"), "html",bold = T)) %>%
  select(class, everything(.)) %>%
  kable("html", escape = F, rownames = T, align=c('r', rep('c', 12))) %>%
  kable_styling("hover", full_width = F)
```

<br>

**Respective Accuracy**
```{r, message=FALSE, warning=FALSE}
print(cm$overall[1:4])
```

-----------------


#### Variable Importance

As determined before, model parameters *minimal node size* and *variables per split* have limited influence on model performance. As a consequence it is likely that the choice of predictor variables is important for model performance.  

In model fitting a relative variable importance is calculated to give an impression which predictor variables are valuable and which are less valuable for the prediction process. Nevertheless, correlation between variables is not taken into account.

```{r gridsearch_importance, fig.cap= "Relative variable importance"}
# variable importance
imp <- varImp(rf_fit)

plot(imp)
```

As we can see the importance metric varies between predictor variables, suggesting that the choice of predictor variables very much influences our model. While more predictor variables might add information they are also complicating the model and might even introduce noise. Hence, a reduction of predictor variables can enhance our model.

-----------------


### Feature Selection

To further simplify the prediction model a Recursive Feature Elimitaion (rfe) is applied. This will eliminate worst performing predictor variables (chosen by importance) at each step and keep the best performing variables to end up in a reduced number of predictor variables which perfom best in model prediction.

```{r rfe, message=FALSE, warning=FALSE}
# perform reverse feature selection with all variables
filename = here("output_data/model/rfProfile_all.rds")
if (file.exists(filename)){
  
  rfProfile <- readRDS(filename)
  
  } else{
    # normalize data
    training_recipe <- recipe(class ~ ., data = training) %>%
      step_center(all_predictors()) %>%
      step_scale(all_predictors()) %>%
      step_nzv(all_predictors()) 
    
    train_prepped <- 
      training_recipe %>% 
      prep(training) %>% 
      juice()
    
    # number of features to test
    subsets <- c(1:19)
    
    training_ctrl <- rfeControl(
      method = "repeatedcv",
      repeats = 5,
      functions = rfFuncs, 
      returnResamp = "all"
    )
    
    library(doParallel)
    cl <- makePSOCKcluster(4)
    registerDoParallel(cl)
    
    rfProfile <- rfe(x = train_prepped %>% dplyr::select(-class),
                     y = train_prepped$class,
                     sizes = subsets,
                     metric = "Accuracy",
                     rfeControl = training_ctrl)
    
    stopCluster(cl)
    
    # save model
    saveRDS(rfProfile, filename)
    
    }
```

-----------------

#### Number of Features

The best model in regards to predictor variables uses 18 out of 19 variables, all except for *VV_variance*. However, we can see that the model performs equally good with less predictor variables.
```{r rfe_result2, echo=FALSE, fig.cap= "Model performance by number of features evaluated with Recursive Feature Elimitaion"}
#ggplot(rfProfile) + theme_classic()
plot_ly(rfProfile$resample, y = ~Accuracy, color = ~as.factor(Variables), colors= viridis_pal(option = "D")(19), type = "box")
 
```


**Chosen variables**

```{r rfe_result, echo=FALSE}
print(predictors(rfProfile)) # chosen predictors
```

-----------------

#### Model Simplification

To simplify the model without loosing prediction accuracy we search for a model with less predictor variables, which has the same accuracy as the best performing model (max 2 % difference in accuracy).

```{r rfe_result3, echo=FALSE}
# get simplest model with similar accuracy
whichTwoPct <- tolerance(rfProfile$results, metric = "Accuracy", 
                         tol = 2, maximize = TRUE)  
var_num <- rfProfile$results[whichTwoPct,"Variables"]
rfProfile$results[whichTwoPct,]

```

A model using the following 8 prediction variables instead of all 18 variables has almost the same accuracy.

```{r rfe_result4}
selectedVars <- rfProfile$variables
bestVar <- rfProfile$control$functions$selectVar(selectedVars, var_num)
bestVar
```

-----------------

### Final Model

Using the results from previous analysis we train a model with best performing predictor variables and model-hyperparameters. 

The predictor variables are:

* VH
* B5
* VV:VH
* B11
* VV
* B12
* NBRI
* NDVI

The hyperparameters are:

* Number of variables to possibly split at in each node (mtry) = 2
* Minimal node size = 1
* Number of trees = 500 (this was not optimized, as more trees usually give better results)

-----------------

#### Model Validation

Applying the final model to predict LuLc classes for the validation data set, the error matrix looks like this: 

<br>

**Error Matrix**
```{r rfe_result5, message=FALSE, warning=FALSE}
library(ranger)

f <- as.formula(paste("class", paste(bestVar, collapse=" + "), sep=" ~ "))

simpler_model <- ranger(formula = f, 
                        data = training,
                        num.trees = 500, 
                        mtry = 2,
                        min.node.size = 1,
                        importance = "impurity")

testing.pred <- predict(simpler_model, testing)

cm<- confusionMatrix(data = testing.pred$predictions, reference = testing$class)

cm.df <- as.data.frame.matrix(cm$table)

cm.df %>%
  
  mutate_all(~ifelse(. > max(cm.df[1:12])*0.5,
                  cell_spec(., "html", color = "white", bold = T, background = "green"),
                  ifelse(. > 0,
                         cell_spec(., color = "white", bold = T, background = spec_color(., option = "A", direction= -1, begin = 0.3, scale_from = c(0,53))),
                         cell_spec(., "html", color = "grey")
                         )
                  )
             ) %>%
  
 mutate(class = cell_spec(c("water - 0","primary forest - 1","secondary forest - 2","mature oilpalm - 3", "immature oilpalm - 4","shrubland - 5","plantation forest - 6","burned / cleared - 7","urban buildings - 8","coconut plantation - 10","rice - 11","tea plantation - 12"), "html",bold = T)) %>%
  select(class, everything(.)) %>%
  kable("html", escape = F, rownames = T, align=c('r', rep('c', 12))) %>%
  kable_styling("hover", full_width = F) 

```

<br>

**Respective Accuracy**
```{r rfe_result6, message=FALSE, warning=FALSE}
print(cm$overall[1:4])
```

If we compare the accuracy calculated for this model with the accuracy calculated for the previous model (using all predictor variables) we can see that it dropped by more than the defined 2%. Why is this?

- One reason might be that we optimized the model parameters (*minimal node size* and *variables per split*) for a model using all predictor variables. To check this we should apply the gridsearch to our simplified model.

- Another reason might be that we calculated model performance based on cross-validated traing data in model tuning, while we used validation data for calculating the error matrices. Hence, using all predictor variables might result in overfitting on training data.

### Classification Reliability

The model validation provides us with values for overall model performance, but cannot give us regional performance values.
Therefore a second classification was produced to inform about per pixel classification certainty. Since classification probability can only be calculated for a two-class classifier and the project focus is put on oilpalm plantations, the LuLc-classes in the training dataset were merged into following classes: 

1. **Oilpalm plantation (mature and immature)** 
2. **other classes**

A classifier was then trained on this data with its output mode set to probability.

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/presentation/certainty.jpg?raw=true")
```
[see in GEE](https://code.earthengine.google.com/ae39bc061d4b551c20720a432d7b02e9)

**dark purple** - high certainty for **other classes**

**yellow** - high certainty for **oilpalm plantation**

**pink/red** - low certainty for **any class**

-----------------

# Results

Using the final model from previous steps we perform the classification itself in GEE. 

[`r icon::fa("code")` Code](https://wiesehahn.github.io/crc990_b05/ee_scripts.html#13_export_of_classification)

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/presentation/results_banner.jpg?raw=true")
```
[see in GEE](https://code.earthengine.google.com/f928cbe0eb9c6679c3fad8efb428306e)

-----------------

# Problems 

## Classification

### reference data {.tabset .tabset-pills}

#### bad image quality {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/problems/ge_bad-scene.jpg?raw=true")
```
[see in Google Maps](https://www.google.com/maps/@-1.358113,102.9531757,4277m/data=!3m1!1e3)

For a lot of areas in Jambi there is no appropriate reference image available. Problems, among others, are:  
- no high resolution imagery is available  
- high resolution imagery is covered by clouds  
- last appropriate image is severeal years old  

#### land-use mosaic 1 {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/problems/ge_mosaic1.jpg?raw=true")
```
[see in Google Maps](https://www.google.com/maps/@-1.1448537,104.1225995,1069m/data=!3m1!1e3)

In many regions of Jambi province, the lulc-pattern is very heterogenious with many different classes densely mixed. In these cases it is hard to collect reference data because lulc areas covered by a single class are very small and chances are high to receive backscatter of mixed classes.

#### land-use mosaic 2 {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/problems/ge_mosaic2.jpg?raw=true")
```
[see in Google Maps](https://www.google.com/maps/@-1.7708193,101.2165027,881m/data=!3m1!1e3)

In some cases lulc classes are devided by other classes (e.g. intercropped with trees) which also makes it more difficult to collect proper reference data.

#### similar appearance {-}
```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics("https://github.com/wiesehahn/crc990_b05/blob/master/output_data/img/reference/problems/ge_plantation.jpg?raw=true")
```
[see in Google Maps](https://www.google.com/maps/@-1.396042,102.7611715,935m/data=!3m1!1e3)

Some classes are hard to differentiate from each other in aerial images. For example:  
- Primary forest - Secondary forest  
- Secondary forest - Plantation forest  
- Oil palm plantation - Coconut plantation  

### Processing

Although most processing steps were done in Google Earth Engine, using high performance server clusters via cloud computing, calculations were problematic. For the calculations vast amounts of data had to be processed. The area of interest (Jambi) is quite large covering many image tiles, for each tile data from almost 2 years was processed from different sources (Sentinel-1 and Sentinel-2) in many steps. Due to these complex processing steps calculations were aborted (timed-out) from time to time and other calculations took several days to complete before results could be evaluated and following steps could be initiated. 


-----------------

# References

